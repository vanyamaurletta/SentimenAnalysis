# -*- coding: utf-8 -*-
"""SentimentAnalysis_LinkAja.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ot25jVtzJUC2ZJQlSCrkuU3XeRr1ZflW

# **Libraries**
"""

# installation
!pip install nltk
!pip install google-play-scraper
!pip install scikit-learn

# import libraries

#NLTK
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords

# GOOGLE PLAY SCRAPER
from google_play_scraper import reviews_all, reviews
from google_play_scraper import app

# DATA MANIPULATION and VISUALIZATION
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS


# MACHINE LEARNING
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score

"""# **Get Data**"""

result, continuation_token = reviews(
    'com.telkom.mwallet', #web id
    lang= 'id', # language
    count = 400000
    )

# Make dataframe
data = pd.DataFrame(np.array(result),columns=['content'])
data = data.join(pd.DataFrame(data.pop('content').tolist()))
data

# Display only review and rating
data = data[['content','score']] # select columns
data = data.rename(columns={'content':'ulasan','score':'nilai'}) # rename columns
data.head()

data.to_csv('ulasanLinkAja.csv') # Datafrane to CSV
dataLinkAja = pd.read_csv('ulasanLinkAja.csv') # Validate read csv

"""# **Labeling**"""

# Labeling data

# Rating 1 and 2 --> label = 0 --> Negative sentiment
# Rating 3,4, and 5 --> label = 1 --> Positive sentiment

dataLinkAja['label'] = dataLinkAja['nilai'].apply(lambda x: 0 if x <= 2 else 1)
labelSentiment_count = dataLinkAja['label'].value_counts()
labelSentiment_count

# Count label
label_counts = dataLinkAja['label'].value_counts().sort_index()
label_names = ['Negative', 'Positive']

# Label visualization
plt.figure(figsize=(8, 8))
plt.pie(label_counts, labels=label_names, autopct='%1.1f%%', colors=['#FFB6C1','#77DD77'])
plt.title("Sentiment Label Distribution")

"""# **Preprocessing**"""

dataLinkAja['ulasan'] = dataLinkAja['ulasan'].str.lower() # Lowercasing
dataLinkAja['ulasan'] = dataLinkAja['ulasan'].str.replace(r'[^\w\s]', ' ', regex=True) # Remove punctuation

# Slang word handling

# Define normalization for slang word
normText = {
    "gak" : "tidak",
    "gpp" : "tidak apa",
    "dg" : "dengan",
    "ngga" : "tidak",
    "dgn" : "dengan",
    "tdk" : "tidak",
    "blm" : "belum",
    "mantap" : "bagus",
    "bgs" : "bagus",
    "gx" : "tidak",
    "sampe" : "sampai",
    "cepet" : "cepat",
    "bgt" : "sangat",
    "banget" : "sangat",
    "super" : "bagus",
    "paling" : "sangat",
    "bagu" : "bagus",
    "baguss" : "bagus"
}

# Normalization function
def normalisasi(slang_word):

  slang_word = str(slang_word)
  # Slang word checker
  for i in normText:
    slang_word = slang_word.replace(i, normText[i])

  return slang_word

# Apply function to ulasan column
dataLinkAja['ulasan'] = dataLinkAja['ulasan'].apply(lambda x: normalisasi(x))

# Tokenization
dataLinkAja['tokens'] = dataLinkAja['ulasan'].apply(lambda x:x.split())

# Stopwords removal

# Set stopwords for indonesian language
all_stopwords = stopwords.words('indonesian')

# Define additional stopwords (optional)
additional_stopwords = ['nya', 'di', 'yg',
                        'yang', 'tolong', 'link',
                        'aja', 'linkaja', 'aplikasi',
                        'ini', 'ga', 'gk', 'udah', 'bisa', 'ya']
all_stopwords.extend(additional_stopwords)

# Lemmatization

def lemmatize_text(token_list):
    return " ".join([lemmatizer.lemmatize(token) for token in token_list if not token in set(all_stopwords)])

lemmatizer = nltk.stem.WordNetLemmatizer()
dataLinkAja['ulasan_clean'] = dataLinkAja['tokens'].apply(lambda x: lemmatize_text(x))

dataLinkAja.head()

"""# **Exploration Sentimen Analysis**"""

# Count rating
rating_counts = dataLinkAja['nilai'].value_counts().sort_index()
rating_labels = ['Rating 1', 'Rating 2', 'Rating 3', 'Rating 4', 'Rating 5']

# Rating visualization
plt.figure(figsize=(8, 8))
colors=['skyblue','pink','lightgreen','violet','coral']
plt.pie(rating_counts, labels=rating_labels, autopct='%1.1f%%', colors=colors)
plt.title("Rating Distribution")
plt.show()

"""**Wordcloud**"""

# Define train_0 for negative sentimen label
train_0 = dataLinkAja[dataLinkAja["label"] == 0]

# Visualization
all_text_0 = ' '.join(word for word in train_0["ulasan_clean"]) # join all reviews

wordcloud = WordCloud(colormap='Reds', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_0)

plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Since LinkAja is an e-wallet service, we want to see further about
# the reviews related to transaction issues

keywords = ['transaksi', 'transfer', 'saldo']

# get reviews that contain keywords
def contain_keywords(text):
    return any(keyword in text for keyword in keywords)

# transaction issues dataframe
transaction_issues = train_0[train_0['ulasan_clean'].apply(contain_keywords)]
transaction_issues = transaction_issues[['ulasan_clean']]

# Keep transaction issues data to CSV
transaction_issues.to_csv('transaction_issues.csv', index=False)

# Transaction issues wordcloud

additional_stopwords = {'saldo', 'top', 'transaksi', 'isi', 'transfer',
                        'pembayaran', 'aplikasi', 'masuk', 'apk','aja', 'ini',
                        'kali','kalo','sy','tp','saya','uang','akun','pulsa'}

text = " ".join(review for review in transaction_issues['ulasan_clean'])

wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS.union(additional_stopwords)).generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

# Define train_1 for positive sentimen label
train_1  = dataLinkAja[dataLinkAja["label"] == 1]

# Visualization
all_text_1 = ' '.join(word for word in train_1["ulasan_clean"]) # join all reviews

wordcloud = WordCloud(colormap='Greens', width=1000, height=1000, mode='RGBA', background_color='white').generate(all_text_1)

plt.figure(figsize=(20, 10))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""# **Machine Learning**"""

# Splitting data

x = dataLinkAja.ulasan_clean
y = dataLinkAja.label

x_train, x_test, y_train, y_test = train_test_split(x,y)

tvec = TfidfVectorizer() # Vectorizer TF-IDF
clf1 = LogisticRegression() # Model algorithm

# Define pipeline
model1 = Pipeline([('vectorizer', tvec), ('classifier', clf1)])

model_1 = model1.fit(x_train,y_train) # Model Training
predict1 = model1.predict(x_test) # Model Predicting

matrix = classification_report(y_test,predict1) # Evaluation report
print("classification_repot: \n", matrix)

# Evaluation metrics
accuracy_model1 = accuracy_score(predict1,y_test)
precision_model1 = precision_score(y_test, predict1, average='weighted')
recall_model1 = recall_score(y_test, predict1, average='weighted')
f1_model1 = f1_score(y_test, predict1, average='weighted')

# Make dataframe
model = {'Model' : ['Logistic Regression'],
         'Accuracy' : [accuracy_model1],
         'Precision' : [precision_model1],
         'Recall' : [recall_model1],
         'F1' : [f1_model1]
         }

df_model = pd.DataFrame(model)
df_model

"""**Use model for prediction**"""

def get_user_reviews(num):
  print("="*100)
  reviews = [input(f"Review {i+1}: ") for i in range(num)]
  return reviews


def predict_and_display_reviews(reviews):
  model = ('Logistic Regression', model_1)
  print("="*100)

  for review in reviews:
    prediction = model[1].predict([review])[0]
    print(f"\nReview: {review}")
    print(f"Predict Review Result with {model[0]}: {prediction}")

# Main program
num_review = int(input("Number of reviews you want to input: "))
user_reviews = get_user_reviews(num_review)
predict_and_display_reviews(user_reviews)